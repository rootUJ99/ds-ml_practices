{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import tensorflow.keras as ks\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow_core.keras in tensorflow_core:\n",
      "\n",
      "NAME\n",
      "    tensorflow_core.keras - Implementation of the Keras API meant to be a high-level API for TensorFlow.\n",
      "\n",
      "DESCRIPTION\n",
      "    Detailed documentation and user guides are available at\n",
      "    [keras.io](https://keras.io).\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    activations (package)\n",
      "    applications (package)\n",
      "    backend (package)\n",
      "    callbacks (package)\n",
      "    constraints (package)\n",
      "    datasets (package)\n",
      "    estimator (package)\n",
      "    experimental (package)\n",
      "    initializers (package)\n",
      "    layers (package)\n",
      "    losses (package)\n",
      "    metrics (package)\n",
      "    mixed_precision (package)\n",
      "    models (package)\n",
      "    optimizers (package)\n",
      "    premade (package)\n",
      "    preprocessing (package)\n",
      "    regularizers (package)\n",
      "    utils (package)\n",
      "    wrappers (package)\n",
      "\n",
      "VERSION\n",
      "    2.2.4-tf\n",
      "\n",
      "FILE\n",
      "    /Users/ujwal/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/api/_v2/keras/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.__version__\n",
    "help(ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 76, 44, 79, 17, 92, 62, 94, 48, 86, 45, 94, 37, 65, 51, 78, 63, 76, 13, 73, 60, 75, 60, 95, 57, 94, 63, 79, 39, 99, 64, 93, 14, 67, 41, 96, 56, 81, 27, 87, 60, 96, 53, 98, 40, 88, 16, 81, 19, 76, 54, 98, 51, 95, 29, 86, 19, 69, 24, 75, 33, 74, 64, 67, 50, 78, 53, 78, 16, 65, 39, 66, 43, 76, 27, 88, 49, 94, 45, 79, 43, 80, 51, 85, 53, 90, 30, 70, 25, 87, 55, 87, 26, 80, 21, 88, 53, 74, 55, 99, 20, 65, 49, 87, 17, 93, 43, 90, 44, 67, 63, 97, 24, 82, 24, 82, 47, 84, 19, 81, 45, 84, 61, 74, 16, 84, 61, 91, 38, 88, 56, 97, 30, 79, 52, 70, 27, 74, 19, 72, 13, 80, 32, 81, 38, 83, 60, 83, 15, 90, 35, 88, 43, 69, 59, 88, 16, 99, 49, 69, 53, 72, 15, 91, 31, 71, 13, 71, 44, 66, 17, 77, 32, 68, 31, 79, 34, 84, 20, 75, 33, 74, 19, 80, 27, 72, 42, 80, 46, 66, 34, 67, 57, 73, 63, 80, 52, 72, 53, 75, 54, 82, 27, 67, 16, 70, 63, 91, 18, 99, 22, 72, 55, 74, 64, 65, 64, 90, 23, 72, 25, 97, 20, 79, 61, 85, 43, 71, 26, 70, 20, 77, 38, 76, 32, 90, 27, 74, 24, 99, 53, 65, 19, 93, 60, 100, 18, 96, 47, 65, 41, 75, 55, 95, 26, 99, 60, 95, 35, 66, 31, 83, 18, 93, 58, 97, 22, 87, 46, 87, 13, 94, 38, 100, 57, 65, 35, 74, 19, 83, 13, 66, 19, 100, 30, 70, 40, 85, 32, 70, 58, 65, 39, 83, 14, 96, 23, 84, 51, 91, 53, 93, 61, 91, 62, 90, 51, 85, 57, 77, 21, 89, 43, 78, 47, 70, 62, 99, 46, 71, 27, 65, 51, 70, 48, 95, 36, 82, 57, 80, 47, 72, 34, 71, 38, 96, 58, 90, 24, 79, 29, 78, 16, 83, 33, 96, 23, 80, 22, 75, 52, 94, 18, 76, 54, 94, 62, 80, 35, 66, 51, 90, 53, 89, 27, 68, 37, 69, 58, 92, 43, 93, 25, 93, 48, 99, 18, 74, 18, 78, 15, 88, 46, 82, 38, 92, 44, 65, 62, 94, 44, 67, 42, 85, 50, 79, 53, 89, 38, 90, 40, 97, 15, 91, 21, 78, 20, 89, 39, 79, 33, 83, 54, 98, 41, 77, 39, 96, 22, 71, 46, 71, 24, 65, 48, 96, 19, 70, 27, 96, 22, 92, 60, 79, 39, 83, 55, 86, 49, 93, 24, 70, 33, 91, 31, 100, 26, 79, 61, 76, 42, 73, 20, 96, 63, 93, 39, 71, 60, 87, 46, 78, 15, 91, 61, 98, 39, 72, 47, 75, 25, 70, 44, 93, 42, 82, 33, 84, 14, 74, 63, 98, 38, 67, 37, 99, 27, 96, 44, 85, 49, 83, 47, 79, 58, 85, 60, 97, 46, 79, 45, 84, 62, 68, 61, 79, 29, 66, 19, 90, 28, 85, 34, 82, 28, 84, 14, 92, 27, 82, 47, 83, 43, 100, 52, 74, 25, 94, 49, 71, 25, 77, 63, 94, 53, 66, 49, 91, 35, 98, 50, 79, 59, 71, 17, 80, 41, 96, 17, 75, 16, 77, 40, 78, 62, 66, 24, 87, 47, 100, 16, 90, 58, 67, 53, 75, 19, 68, 42, 71, 62, 88, 13, 69, 16, 69, 42, 65, 38, 69, 55, 74, 30, 100, 25, 83, 42, 88, 14, 70, 52, 91, 64, 79, 54, 65, 60, 82, 26, 98, 53, 70, 51, 76, 61, 93, 20, 82, 54, 82, 45, 75, 24, 71, 19, 89, 28, 90, 29, 90, 30, 84, 43, 73, 20, 86, 24, 82, 40, 95, 32, 97, 39, 70, 19, 88, 52, 78, 52, 65, 19, 66, 35, 68, 43, 65, 44, 95, 13, 90, 59, 86, 54, 100, 16, 95, 39, 92, 16, 97, 23, 72, 38, 80, 49, 92, 33, 65, 59, 89, 37, 100, 26, 67, 37, 100, 30, 67, 47, 92, 31, 68, 15, 97, 58, 91, 48, 83, 18, 81, 46, 94, 16, 95, 31, 97, 60, 84, 23, 85, 19, 74, 44, 78, 47, 76, 60, 77, 28, 90, 60, 95, 43, 82, 18, 72, 49, 73, 16, 91, 14, 84, 60, 93, 40, 68, 22, 82, 55, 86, 56, 79, 57, 73, 34, 71, 38, 65, 43, 76, 35, 84, 30, 76, 21, 88, 54, 72, 58, 95, 23, 85, 29, 70, 55, 95, 42, 69, 23, 92, 26, 84, 15, 86, 41, 93, 33, 97, 52, 98, 30, 89, 17, 100, 39, 85, 36, 88, 25, 74, 51, 86, 18, 87, 53, 74, 53, 83, 35, 74, 23, 98, 44, 92, 42, 95, 44, 66, 34, 86, 38, 66, 28, 78, 41, 77, 49, 72, 23, 66, 58, 100, 51, 93, 54, 69, 34, 90, 23, 99, 50, 94, 26, 72, 35, 82, 39, 86, 60, 76, 31, 77, 17, 80, 33, 65, 63, 95, 49, 66, 18, 78, 50, 68, 27, 96, 33, 74, 27, 83, 63, 66, 59, 98, 46, 74, 17, 74, 47, 92, 19, 73, 63, 97, 40, 72, 28, 75, 53, 75, 28, 81, 31, 77, 40, 86, 25, 76, 13, 81, 33, 98, 31, 90, 41, 91, 58, 87, 52, 93, 38, 79, 48, 72, 55, 70, 52, 74, 39, 73, 55, 79, 37, 94, 44, 92, 30, 71, 31, 91, 48, 88, 56, 78, 60, 72, 54, 91, 42, 97, 27, 86, 36, 74, 40, 81, 61, 68, 50, 89, 54, 76, 15, 92, 18, 95, 40, 79, 50, 72, 14, 71, 20, 72, 46, 79, 46, 66, 64, 69, 26, 83, 29, 98, 57, 76, 41, 80, 51, 92, 30, 84, 37, 66, 42, 84, 23, 65, 20, 90, 45, 85, 38, 95, 27, 91, 24, 95, 52, 76, 21, 73, 38, 93, 45, 66, 17, 97, 31, 85, 24, 69, 62, 88, 30, 80, 14, 91, 53, 86, 21, 86, 20, 73, 38, 84, 43, 78, 43, 79, 24, 78, 26, 77, 15, 74, 23, 79, 14, 99, 32, 80, 48, 91, 52, 68, 49, 69, 23, 68, 62, 75, 57, 83, 34, 72, 48, 67, 63, 89, 56, 74, 39, 66, 56, 84, 47, 95, 34, 82, 15, 93, 27, 93, 44, 69, 56, 73, 27, 83, 25, 69, 64, 77, 42, 81, 36, 74, 13, 76, 26, 98, 41, 87, 13, 99, 15, 68, 28, 79, 45, 80, 22, 85, 24, 66, 61, 86, 64, 78, 22, 97, 17, 100, 61, 100, 58, 76, 13, 98, 60, 67, 61, 88, 14, 83, 41, 76, 42, 77, 24, 75, 41, 84, 41, 74, 56, 88, 62, 81, 48, 95, 37, 68, 21, 81, 50, 98, 27, 70, 39, 91, 43, 77, 44, 87, 49, 67, 39, 75, 39, 74, 47, 91, 61, 82, 62, 80, 64, 86, 59, 67, 18, 76, 42, 75, 45, 67, 46, 72, 52, 93, 46, 85, 39, 93, 36, 91, 64, 99, 42, 76, 51, 96, 50, 69, 22, 74, 15, 98, 63, 81, 15, 96, 32, 74, 58, 65, 51, 97, 22, 86, 27, 72, 22, 85, 13, 95, 36, 87, 42, 95, 23, 78, 26, 83, 60, 76, 22, 96, 33, 77, 26, 72, 33, 76, 57, 75, 42, 94, 42, 65, 13, 83, 15, 89, 35, 85, 28, 67, 32, 83, 23, 94, 27, 93, 38, 80, 24, 77, 39, 96, 24, 86, 55, 86, 46, 70, 38, 69, 64, 75, 43, 76, 24, 79, 44, 85, 17, 69, 16, 78, 33, 95, 32, 81, 27, 69, 18, 82, 39, 77, 19, 75, 34, 100, 18, 76, 14, 74, 14, 65, 49, 66, 32, 72, 57, 72, 54, 94, 26, 84, 34, 73, 37, 67, 31, 81, 15, 74, 53, 77, 20, 80, 43, 91, 54, 74, 28, 65, 35, 66, 34, 71, 46, 98, 19, 91, 37, 78, 29, 97, 13, 65, 14, 91, 50, 90, 40, 65, 38, 72, 41, 89, 39, 97, 27, 86, 51, 99, 18, 82, 26, 85, 15, 68, 64, 81, 27, 89, 54, 74, 23, 92, 21, 93, 62, 100, 56, 65, 14, 91, 28, 71, 30, 68, 40, 78, 60, 75, 28, 88, 43, 88, 14, 78, 62, 87, 13, 87, 16, 75, 44, 99, 57, 83, 34, 78, 60, 66, 63, 71, 60, 72, 38, 94, 45, 65, 36, 66, 41, 99, 56, 94, 45, 85, 61, 93, 38, 75, 14, 83, 46, 98, 48, 92, 22, 67, 56, 75, 21, 68, 56, 83, 46, 81, 37, 67, 25, 66, 35, 89, 43, 91, 42, 82, 20, 90, 56, 74, 53, 88, 38, 66, 27, 92, 34, 77, 53, 81, 19, 98, 49, 68, 47, 72, 59, 69, 17, 98, 17, 72, 54, 66, 17, 68, 46, 84, 54, 67, 54, 77, 14, 91, 14, 78, 33, 95, 56, 65, 18, 79, 44, 89, 48, 70, 57, 84, 40, 76, 15, 90, 53, 91, 41, 87, 30, 68, 47, 70, 40, 99, 41, 94, 64, 81, 17, 72, 60, 98, 13, 100, 13, 91, 57, 81, 26, 80, 50, 83, 41, 86, 48, 82, 22, 84, 51, 78, 15, 80, 57, 74, 57, 79, 24, 98, 45, 90, 32, 88, 46, 92, 45, 70, 59, 79, 18, 76, 15, 78, 48, 79, 51, 94, 17, 96, 55, 69, 50, 86, 52, 87, 22, 96, 24, 84, 24, 72, 39, 78, 31, 81, 59, 80, 16, 65, 27, 82, 38, 90, 60, 81, 31, 100, 58, 83, 33, 91, 16, 94, 63, 76, 22, 97, 20, 79, 35, 77, 55, 85, 19, 74, 46, 97, 61, 97, 63, 74, 58, 69, 25, 71, 45, 67, 24, 94, 13, 75, 54, 94, 58, 98, 49, 66, 13, 89, 28, 100, 21, 83, 41, 67, 24, 89, 51, 97, 38, 71, 15, 92, 56, 93, 25, 88, 41, 92, 49, 92, 34, 85, 37, 74, 37, 65, 15, 95, 59, 70, 28, 73, 21, 89, 29, 75, 41, 83, 22, 89, 28, 81, 46, 85, 42, 68, 64, 93, 42, 91, 63, 68, 36, 95, 45, 74, 16, 97, 47, 78, 26, 90, 33, 66, 57, 66, 37, 90, 26, 70, 36, 92, 18, 70, 15, 80, 31, 81, 45, 94, 31, 77, 19, 96, 20, 94, 17, 87, 48, 78, 60, 87, 64, 68, 34, 89, 39, 93, 60, 93, 38, 74, 58, 66, 51, 68, 63, 69, 28, 78, 52, 67, 40, 67, 17, 67, 62, 87, 33, 80, 64, 97, 49, 98, 32, 65, 15, 72, 28, 94, 59, 86, 14, 73, 22, 81, 15, 87, 43, 74, 46, 91, 19, 90, 14, 78, 56, 93, 13, 85, 63, 79, 62, 86, 50, 83, 23, 71, 41, 83, 62, 90, 38, 73, 48, 78, 51, 93, 59, 71, 64, 80, 41, 91, 27, 80, 36, 88, 35, 99, 44, 89, 52, 99, 50, 71, 34, 81, 31, 95, 17, 82, 45, 75, 20, 95, 14, 75, 33, 76, 64, 69, 24, 76, 54, 90, 33, 78, 37, 93, 38, 75, 13, 93, 62, 70, 26, 78, 16, 93, 14, 92, 40, 99, 23, 79, 13, 91, 15, 100, 39, 76, 17, 79, 50, 95, 53, 69, 39, 82, 20, 98, 14, 85, 26, 69, 32, 95, 52, 67, 42, 86, 23, 85, 37, 65, 44, 77, 23, 83, 34, 92, 49, 72, 24, 99, 13, 82, 23, 65, 36, 83, 43, 82, 52, 68, 59, 95, 64, 71, 58, 76, 26, 77, 61, 84, 59, 71, 28, 83, 25, 86, 27, 86, 28, 96, 37, 72, 44, 85, 13, 73, 63, 93, 63, 75, 61, 100, 55, 100, 44, 74, 15, 66, 18, 100, 57, 71, 45, 89, 34, 98, 25, 100, 52, 90, 54, 84, 38, 76, 56, 97, 17, 67, 33, 75, 57, 71, 23, 99, 22, 96, 52, 67, 37, 82, 36, 65, 63, 74, 25, 100, 52, 99, 64, 67, 15, 91, 64, 74, 37, 66, 55, 73, 36, 68, 52, 97, 55, 71, 39, 96] [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "test_samples = []\n",
    "test_labels = []\n",
    "for i in range(1000):\n",
    "    random_younger = rd.randint(13,64)\n",
    "    test_samples.append(random_younger)\n",
    "    test_labels.append(1)\n",
    "    \n",
    "    random_older = rd.randint(65,100)\n",
    "    test_samples.append(random_older)\n",
    "    test_labels.append(0)\n",
    "# help(rd)\n",
    "print(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = np.array(test_samples)\n",
    "test_labels = np.array(test_labels)\n",
    "test_samples, test_labels = shuffle(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37 93 65 ... 57 34 39] [1 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(test_samples, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_test_samples = scaler.fit_transform(test_samples.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(units=16, input_shape=(1,), activation=\"relu\"),\n",
    "    Dense(units=32, activation=\"relu\"),\n",
    "    Dense(units=32, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 16)                32        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                1056      \n",
      "=================================================================\n",
      "Total params: 1,632\n",
      "Trainable params: 1,632\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_test_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/50\n",
      "1800/1800 - 1s - loss: 3.4092 - accuracy: 0.1161 - val_loss: 3.3393 - val_accuracy: 0.1600\n",
      "Epoch 2/50\n",
      "1800/1800 - 0s - loss: 3.2575 - accuracy: 0.4567 - val_loss: 3.1463 - val_accuracy: 0.8200\n",
      "Epoch 3/50\n",
      "1800/1800 - 0s - loss: 3.0221 - accuracy: 0.7578 - val_loss: 2.8518 - val_accuracy: 0.7950\n",
      "Epoch 4/50\n",
      "1800/1800 - 0s - loss: 2.6739 - accuracy: 0.6978 - val_loss: 2.4307 - val_accuracy: 0.6750\n",
      "Epoch 5/50\n",
      "1800/1800 - 0s - loss: 2.2098 - accuracy: 0.5778 - val_loss: 1.9173 - val_accuracy: 0.5650\n",
      "Epoch 6/50\n",
      "1800/1800 - 0s - loss: 1.7121 - accuracy: 0.5022 - val_loss: 1.4458 - val_accuracy: 0.5450\n",
      "Epoch 7/50\n",
      "1800/1800 - 0s - loss: 1.3127 - accuracy: 0.4950 - val_loss: 1.1224 - val_accuracy: 0.5450\n",
      "Epoch 8/50\n",
      "1800/1800 - 0s - loss: 1.0606 - accuracy: 0.4950 - val_loss: 0.9355 - val_accuracy: 0.5450\n",
      "Epoch 9/50\n",
      "1800/1800 - 0s - loss: 0.9152 - accuracy: 0.4950 - val_loss: 0.8284 - val_accuracy: 0.5450\n",
      "Epoch 10/50\n",
      "1800/1800 - 0s - loss: 0.8290 - accuracy: 0.4950 - val_loss: 0.7630 - val_accuracy: 0.5450\n",
      "Epoch 11/50\n",
      "1800/1800 - 0s - loss: 0.7738 - accuracy: 0.4950 - val_loss: 0.7211 - val_accuracy: 0.5450\n",
      "Epoch 12/50\n",
      "1800/1800 - 0s - loss: 0.7358 - accuracy: 0.4950 - val_loss: 0.6903 - val_accuracy: 0.5450\n",
      "Epoch 13/50\n",
      "1800/1800 - 0s - loss: 0.7076 - accuracy: 0.4950 - val_loss: 0.6672 - val_accuracy: 0.5450\n",
      "Epoch 14/50\n",
      "1800/1800 - 0s - loss: 0.6850 - accuracy: 0.5067 - val_loss: 0.6476 - val_accuracy: 0.5650\n",
      "Epoch 15/50\n",
      "1800/1800 - 0s - loss: 0.6661 - accuracy: 0.5461 - val_loss: 0.6319 - val_accuracy: 0.6150\n",
      "Epoch 16/50\n",
      "1800/1800 - 0s - loss: 0.6491 - accuracy: 0.6161 - val_loss: 0.6155 - val_accuracy: 0.6550\n",
      "Epoch 17/50\n",
      "1800/1800 - 0s - loss: 0.6335 - accuracy: 0.6539 - val_loss: 0.6012 - val_accuracy: 0.6950\n",
      "Epoch 18/50\n",
      "1800/1800 - 0s - loss: 0.6187 - accuracy: 0.6850 - val_loss: 0.5872 - val_accuracy: 0.7050\n",
      "Epoch 19/50\n",
      "1800/1800 - 0s - loss: 0.6041 - accuracy: 0.7083 - val_loss: 0.5735 - val_accuracy: 0.7650\n",
      "Epoch 20/50\n",
      "1800/1800 - 0s - loss: 0.5898 - accuracy: 0.7294 - val_loss: 0.5593 - val_accuracy: 0.7950\n",
      "Epoch 21/50\n",
      "1800/1800 - 0s - loss: 0.5752 - accuracy: 0.7744 - val_loss: 0.5438 - val_accuracy: 0.7950\n",
      "Epoch 22/50\n",
      "1800/1800 - 0s - loss: 0.5612 - accuracy: 0.7711 - val_loss: 0.5308 - val_accuracy: 0.8400\n",
      "Epoch 23/50\n",
      "1800/1800 - 0s - loss: 0.5464 - accuracy: 0.7922 - val_loss: 0.5169 - val_accuracy: 0.8700\n",
      "Epoch 24/50\n",
      "1800/1800 - 0s - loss: 0.5315 - accuracy: 0.8211 - val_loss: 0.5008 - val_accuracy: 0.8700\n",
      "Epoch 25/50\n",
      "1800/1800 - 0s - loss: 0.5164 - accuracy: 0.8244 - val_loss: 0.4870 - val_accuracy: 0.8900\n",
      "Epoch 26/50\n",
      "1800/1800 - 0s - loss: 0.5011 - accuracy: 0.8422 - val_loss: 0.4709 - val_accuracy: 0.8900\n",
      "Epoch 27/50\n",
      "1800/1800 - 0s - loss: 0.4859 - accuracy: 0.8522 - val_loss: 0.4560 - val_accuracy: 0.9100\n",
      "Epoch 28/50\n",
      "1800/1800 - 0s - loss: 0.4702 - accuracy: 0.8617 - val_loss: 0.4409 - val_accuracy: 0.9100\n",
      "Epoch 29/50\n",
      "1800/1800 - 0s - loss: 0.4544 - accuracy: 0.8750 - val_loss: 0.4238 - val_accuracy: 0.9100\n",
      "Epoch 30/50\n",
      "1800/1800 - 0s - loss: 0.4388 - accuracy: 0.8867 - val_loss: 0.4080 - val_accuracy: 0.9150\n",
      "Epoch 31/50\n",
      "1800/1800 - 0s - loss: 0.4232 - accuracy: 0.8917 - val_loss: 0.3928 - val_accuracy: 0.9200\n",
      "Epoch 32/50\n",
      "1800/1800 - 0s - loss: 0.4076 - accuracy: 0.8972 - val_loss: 0.3777 - val_accuracy: 0.9300\n",
      "Epoch 33/50\n",
      "1800/1800 - 0s - loss: 0.3921 - accuracy: 0.9072 - val_loss: 0.3619 - val_accuracy: 0.9300\n",
      "Epoch 34/50\n",
      "1800/1800 - 0s - loss: 0.3769 - accuracy: 0.9133 - val_loss: 0.3474 - val_accuracy: 0.9350\n",
      "Epoch 35/50\n",
      "1800/1800 - 0s - loss: 0.3622 - accuracy: 0.9183 - val_loss: 0.3329 - val_accuracy: 0.9350\n",
      "Epoch 36/50\n",
      "1800/1800 - 0s - loss: 0.3474 - accuracy: 0.9228 - val_loss: 0.3187 - val_accuracy: 0.9350\n",
      "Epoch 37/50\n",
      "1800/1800 - 0s - loss: 0.3333 - accuracy: 0.9283 - val_loss: 0.3047 - val_accuracy: 0.9500\n",
      "Epoch 38/50\n",
      "1800/1800 - 0s - loss: 0.3197 - accuracy: 0.9333 - val_loss: 0.2907 - val_accuracy: 0.9500\n",
      "Epoch 39/50\n",
      "1800/1800 - 0s - loss: 0.3068 - accuracy: 0.9311 - val_loss: 0.2797 - val_accuracy: 0.9600\n",
      "Epoch 40/50\n",
      "1800/1800 - 0s - loss: 0.2945 - accuracy: 0.9383 - val_loss: 0.2666 - val_accuracy: 0.9600\n",
      "Epoch 41/50\n",
      "1800/1800 - 0s - loss: 0.2825 - accuracy: 0.9394 - val_loss: 0.2559 - val_accuracy: 0.9600\n",
      "Epoch 42/50\n",
      "1800/1800 - 0s - loss: 0.2712 - accuracy: 0.9428 - val_loss: 0.2450 - val_accuracy: 0.9600\n",
      "Epoch 43/50\n",
      "1800/1800 - 0s - loss: 0.2606 - accuracy: 0.9444 - val_loss: 0.2335 - val_accuracy: 0.9600\n",
      "Epoch 44/50\n",
      "1800/1800 - 0s - loss: 0.2502 - accuracy: 0.9489 - val_loss: 0.2252 - val_accuracy: 0.9600\n",
      "Epoch 45/50\n",
      "1800/1800 - 0s - loss: 0.2406 - accuracy: 0.9450 - val_loss: 0.2164 - val_accuracy: 0.9700\n",
      "Epoch 46/50\n",
      "1800/1800 - 0s - loss: 0.2316 - accuracy: 0.9556 - val_loss: 0.2071 - val_accuracy: 0.9600\n",
      "Epoch 47/50\n",
      "1800/1800 - 0s - loss: 0.2233 - accuracy: 0.9522 - val_loss: 0.1996 - val_accuracy: 0.9700\n",
      "Epoch 48/50\n",
      "1800/1800 - 0s - loss: 0.2150 - accuracy: 0.9556 - val_loss: 0.1943 - val_accuracy: 0.9800\n",
      "Epoch 49/50\n",
      "1800/1800 - 0s - loss: 0.2079 - accuracy: 0.9633 - val_loss: 0.1854 - val_accuracy: 0.9700\n",
      "Epoch 50/50\n",
      "1800/1800 - 0s - loss: 0.2009 - accuracy: 0.9622 - val_loss: 0.1785 - val_accuracy: 0.9700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x147064510>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=scaler_test_samples, y=test_labels, batch_size=10, epochs=50, shuffle=True, verbose=2, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_samples = []\n",
    "new_test_labels = []\n",
    "for i in range(100):\n",
    "    random_younger = rd.randint(13,64)\n",
    "    new_test_samples.append(random_younger)\n",
    "    new_test_samples.append(1)\n",
    "    \n",
    "    random_older = rd.randint(65,100)\n",
    "    new_test_labels.append(random_older)\n",
    "    new_test_labels.append(0)\n",
    "# help(rd)\n",
    "\n",
    "\n",
    "new_test_samples = np.array(new_test_samples)\n",
    "new_test_labels = np.array(new_test_labels)\n",
    "new_test_samples, new_test_labels = shuffle(new_test_samples, new_test_labels)\n",
    "new_scaler_test_samples = scaler.fit_transform(new_test_samples.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x=new_scaler_test_samples, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in rounded_predictions:\n",
    "    print( i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
